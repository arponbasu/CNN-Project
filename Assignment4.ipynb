{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 : Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will design an artificial neural network for a binary classification task. We will use Keras for implementing the neural network in this assignment. You can also use numpy, pandas and/or scikit-learn wherever you find them useful. You'll also need matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the required libraries here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-optimize in /home/arpon/snap/jupyter/common/lib/python3.7/site-packages (0.8.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/arpon/snap/jupyter/common/lib/python3.7/site-packages (from scikit-optimize) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/arpon/snap/jupyter/common/lib/python3.7/site-packages (from scikit-optimize) (1.19.5)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /home/arpon/snap/jupyter/common/lib/python3.7/site-packages (from scikit-optimize) (0.24.1)\n",
      "Requirement already satisfied: pyaml>=16.9 in /home/arpon/snap/jupyter/common/lib/python3.7/site-packages (from scikit-optimize) (20.4.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/arpon/snap/jupyter/common/lib/python3.7/site-packages (from scikit-optimize) (1.6.2)\n",
      "Requirement already satisfied: PyYAML in /home/arpon/snap/jupyter/common/lib/python3.7/site-packages (from pyaml>=16.9->scikit-optimize) (5.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/arpon/snap/jupyter/common/lib/python3.7/site-packages (from scikit-learn>=0.20.0->scikit-optimize) (2.1.0)\n",
      "Library loading complete....\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from sklearn import preprocessing\n",
    "import keras.models\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "! python -m pip install scikit-optimize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "print(\"Library loading complete....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset from the file named 'data.csv' . You'll find 7200 datapoints in this file. The first 6 columns are the features (X), while the last column has a binary label (Y) for each feature vector. After loading the dataset, divide it into a training set and a test set (cross-validation set to be more accurate). You can have 70% datapoints in the training set and 30% in the test set. An 80-20 split is also acceptable.\n",
    "\n",
    "Normalize your training set using mean and variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.30e-01 6.00e-04 1.50e-02 ... 8.20e-02 1.46e-01 0.00e+00]\n",
      " [2.40e-01 2.50e-04 3.00e-02 ... 1.33e-01 1.08e-01 0.00e+00]\n",
      " [4.70e-01 1.90e-03 2.40e-02 ... 1.31e-01 7.80e-02 0.00e+00]\n",
      " ...\n",
      " [5.10e-01 7.60e-04 2.01e-02 ... 6.70e-02 1.34e-01 0.00e+00]\n",
      " [3.50e-01 2.80e-03 2.01e-02 ... 8.90e-02 1.01e-01 0.00e+00]\n",
      " [7.30e-01 5.60e-04 2.01e-02 ... 9.00e-02 9.00e-02 0.00e+00]]\n",
      "##########\n",
      "[[1.00e+00 7.30e-01 6.00e-04 ... 1.20e-01 8.20e-02 1.46e-01]\n",
      " [1.00e+00 2.40e-01 2.50e-04 ... 1.43e-01 1.33e-01 1.08e-01]\n",
      " [1.00e+00 4.70e-01 1.90e-03 ... 1.02e-01 1.31e-01 7.80e-02]\n",
      " ...\n",
      " [1.00e+00 5.10e-01 7.60e-04 ... 9.00e-02 6.70e-02 1.34e-01]\n",
      " [1.00e+00 3.50e-01 2.80e-03 ... 9.00e-02 8.90e-02 1.01e-01]\n",
      " [1.00e+00 7.30e-01 5.60e-04 ... 8.10e-02 9.00e-02 9.00e-02]]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "##########\n",
      "[[1.00e+00 7.30e-01 6.00e-04 ... 1.20e-01 8.20e-02 1.46e-01]\n",
      " [1.00e+00 2.40e-01 2.50e-04 ... 1.43e-01 1.33e-01 1.08e-01]\n",
      " [1.00e+00 4.70e-01 1.90e-03 ... 1.02e-01 1.31e-01 7.80e-02]\n",
      " ...\n",
      " [1.00e+00 3.40e-01 1.40e-03 ... 1.26e-01 1.00e-01 1.26e-01]\n",
      " [1.00e+00 4.40e-01 3.20e-03 ... 8.80e-02 1.02e-01 8.60e-02]\n",
      " [1.00e+00 3.50e-01 8.00e-05 ... 1.30e-01 9.10e-02 1.43e-01]]\n",
      "[[1.00e+00 6.50e-01 2.60e-03 ... 1.42e-01 1.01e-01 1.41e-01]\n",
      " [1.00e+00 6.50e-01 1.30e-03 ... 1.30e-01 1.21e-01 1.07e-01]\n",
      " [1.00e+00 4.40e-01 8.50e-04 ... 8.10e-02 9.60e-02 8.40e-02]\n",
      " ...\n",
      " [1.00e+00 5.10e-01 7.60e-04 ... 9.00e-02 6.70e-02 1.34e-01]\n",
      " [1.00e+00 3.50e-01 2.80e-03 ... 9.00e-02 8.90e-02 1.01e-01]\n",
      " [1.00e+00 7.30e-01 5.60e-04 ... 8.10e-02 9.00e-02 9.00e-02]]\n",
      "##########\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "##########\n",
      "[[7.96676099e-01 5.81573552e-01 4.78005659e-04 ... 9.56011319e-02\n",
      "  6.53274401e-02 1.16314710e-01]\n",
      " [9.49884781e-01 2.27972347e-01 2.37471195e-04 ... 1.35833524e-01\n",
      "  1.26334676e-01 1.02587556e-01]\n",
      " [8.92597951e-01 4.19521037e-01 1.69593611e-03 ... 9.10449910e-02\n",
      "  1.16930332e-01 6.96226402e-02]\n",
      " ...\n",
      " [9.29325103e-01 3.15970535e-01 1.30105514e-03 ... 1.17094963e-01\n",
      "  9.29325103e-02 1.17094963e-01]\n",
      " [9.05439384e-01 3.98393329e-01 2.89740603e-03 ... 7.96786658e-02\n",
      "  9.23548172e-02 7.78677870e-02]\n",
      " [9.25048400e-01 3.23766940e-01 7.40038720e-05 ... 1.20256292e-01\n",
      "  8.41794044e-02 1.32281921e-01]]\n",
      "##########\n"
     ]
    }
   ],
   "source": [
    "M = pd.read_csv(\"data.csv\", header = None) \n",
    "M = np.array(M)\n",
    "print(M)\n",
    "print(\"##########\")\n",
    "\n",
    "\n",
    "split = len(M[0])-1\n",
    "X = M[:,:split]\n",
    "prepend = np.ones([len(X), 1])\n",
    "Xraw = X\n",
    "X = np.hstack((prepend,X))\n",
    "y = M[:,split:]  \n",
    "y = y.flatten()\n",
    "print(X)\n",
    "print(y)\n",
    "print(\"##########\")\n",
    "\n",
    "\n",
    "data = X\n",
    "split = (int)(0.7*len(M))\n",
    "Xtrain,Xtest = data[:split,:],data[split:,:]\n",
    "print(Xtrain)\n",
    "print(Xtest)\n",
    "print(\"##########\")\n",
    "\n",
    "\n",
    "data = y\n",
    "split = (int)(0.7*len(M))\n",
    "Ytrain,Ytest = data[:split,],data[split:,]\n",
    "Ytrain = Ytrain.flatten()\n",
    "Ytest = Ytest.flatten()\n",
    "print(Ytrain)\n",
    "print(Ytest)\n",
    "print(\"##########\")\n",
    "\n",
    "\n",
    "normalizer = preprocessing.Normalizer()\n",
    "normalized_train_X = normalizer.fit_transform(Xtrain)\n",
    "print(normalized_train_X)\n",
    "print(\"##########\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the number of positive and negative samples in the training set /test set /the whole dataset. You'll use this result while evaluating your neural network model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "534\n",
      "6666\n",
      "##########\n",
      "370\n",
      "4670\n",
      "##########\n",
      "164\n",
      "1996\n",
      "##########\n"
     ]
    }
   ],
   "source": [
    "ynegative = 0\n",
    "ypositive = 0\n",
    "for i in range(len(y)):\n",
    "    if(y[i] == 0):\n",
    "                ynegative += 1 \n",
    "                \n",
    "    elif(y[i] == 1): ypositive += 1\n",
    "print(ypositive)\n",
    "print(ynegative)\n",
    "print(\"##########\")\n",
    "\n",
    "\n",
    "Ytrainnegative = 0\n",
    "Ytrainpositive = 0\n",
    "for i in range(len(Ytrain)):\n",
    "    if(Ytrain[i] == 0):\n",
    "                Ytrainnegative += 1 \n",
    "                \n",
    "    elif(Ytrain[i] == 1): Ytrainpositive += 1\n",
    "print(Ytrainpositive)\n",
    "print(Ytrainnegative)\n",
    "print(\"##########\")\n",
    "\n",
    "\n",
    "Ytestnegative = 0\n",
    "Ytestpositive = 0\n",
    "for i in range(len(Ytest)):\n",
    "    if(Ytest[i] == 0):\n",
    "                Ytestnegative += 1 \n",
    "                \n",
    "    elif(Ytest[i] == 1): Ytestpositive += 1\n",
    "print(Ytestpositive)\n",
    "print(Ytestnegative)\n",
    "print(\"##########\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the architecture of the neural network\n",
    "\n",
    "Initialise a sequential neural network model using keras.models.Sequential(), and add dense layers (dense layer means fully-connected layer) to it using keras.layers.Dense() (you easily how to do this from the internet).\n",
    "\n",
    "Use ReLU activation function in every layer, except the last one, where you'll use the sigmoid activation function, since it's a binary classification task.\n",
    "\n",
    "The choice of the number of layers and the number of units in a layer is totally up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(24, input_dim=len(normalized_train_X[0]), activation='relu'))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the neural network\n",
    "\n",
    "1. Compile your nn model using model.compile() . Use the appropriate loss function (binary cross-entropy), and use Adam optimizer. Pass on 'accuracy' as a metric, so that you get to see the accuracy on your training set after every iteration of Adam optimization (a form of mini-batch gradient descent).\n",
    "\n",
    "Try to look-up and learn a bit about what stochastic gradient descent and mini-batch gradient descent essentilly are. You'll use mini-batches while training your model.\n",
    "\n",
    "2. Train your model using model.fit() (this will take a while, perhaps a few minutes). Use appropriate number of ephocs and batch size (you have to decide which values work the best). Don't forget that you have to train your model on the training set, and not the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "153/153 [==============================] - 2s 2ms/step - loss: 0.6164 - accuracy: 0.5970\n",
      "Epoch 2/150\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 0.2638 - accuracy: 0.9235\n",
      "Epoch 3/150\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 0.2569 - accuracy: 0.9247\n",
      "Epoch 4/150\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 0.2378 - accuracy: 0.9287\n",
      "Epoch 5/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.2368 - accuracy: 0.9247\n",
      "Epoch 6/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.2039 - accuracy: 0.9360\n",
      "Epoch 7/150\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 0.2044 - accuracy: 0.9356\n",
      "Epoch 8/150\n",
      "153/153 [==============================] - 1s 4ms/step - loss: 0.1918 - accuracy: 0.9409\n",
      "Epoch 9/150\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 0.1898 - accuracy: 0.9369\n",
      "Epoch 10/150\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 0.1788 - accuracy: 0.9427\n",
      "Epoch 11/150\n",
      "153/153 [==============================] - 1s 4ms/step - loss: 0.1901 - accuracy: 0.9387\n",
      "Epoch 12/150\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 0.1769 - accuracy: 0.9429\n",
      "Epoch 13/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1673 - accuracy: 0.9458\n",
      "Epoch 14/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1734 - accuracy: 0.9429\n",
      "Epoch 15/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1636 - accuracy: 0.9457\n",
      "Epoch 16/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1707 - accuracy: 0.9457\n",
      "Epoch 17/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1676 - accuracy: 0.9475\n",
      "Epoch 18/150\n",
      "153/153 [==============================] - 1s 4ms/step - loss: 0.1596 - accuracy: 0.9502\n",
      "Epoch 19/150\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.1677 - accuracy: 0.9459\n",
      "Epoch 20/150\n",
      "153/153 [==============================] - 1s 3ms/step - loss: 0.1551 - accuracy: 0.9530\n",
      "Epoch 21/150\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 0.1508 - accuracy: 0.9544\n",
      "Epoch 22/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1544 - accuracy: 0.9507\n",
      "Epoch 23/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1545 - accuracy: 0.9527\n",
      "Epoch 24/150\n",
      "153/153 [==============================] - 1s 4ms/step - loss: 0.1702 - accuracy: 0.9467\n",
      "Epoch 25/150\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.1597 - accuracy: 0.9462\n",
      "Epoch 26/150\n",
      "153/153 [==============================] - 1s 6ms/step - loss: 0.1611 - accuracy: 0.9458\n",
      "Epoch 27/150\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1575 - accuracy: 0.9477\n",
      "Epoch 28/150\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.1548 - accuracy: 0.9499\n",
      "Epoch 29/150\n",
      "153/153 [==============================] - 1s 4ms/step - loss: 0.1651 - accuracy: 0.9471\n",
      "Epoch 30/150\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 0.1481 - accuracy: 0.9488\n",
      "Epoch 31/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1494 - accuracy: 0.9509\n",
      "Epoch 32/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1465 - accuracy: 0.9522\n",
      "Epoch 33/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1381 - accuracy: 0.9544\n",
      "Epoch 34/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1453 - accuracy: 0.9529\n",
      "Epoch 35/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1300 - accuracy: 0.9578\n",
      "Epoch 36/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1460 - accuracy: 0.9508\n",
      "Epoch 37/150\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 0.1399 - accuracy: 0.9546\n",
      "Epoch 38/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1458 - accuracy: 0.9501\n",
      "Epoch 39/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1389 - accuracy: 0.9514\n",
      "Epoch 40/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1431 - accuracy: 0.9484\n",
      "Epoch 41/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1444 - accuracy: 0.9515\n",
      "Epoch 42/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1221 - accuracy: 0.9576\n",
      "Epoch 43/150\n",
      "153/153 [==============================] - 1s 4ms/step - loss: 0.1146 - accuracy: 0.9602\n",
      "Epoch 44/150\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 0.1211 - accuracy: 0.9536\n",
      "Epoch 45/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1234 - accuracy: 0.9522\n",
      "Epoch 46/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1303 - accuracy: 0.9529\n",
      "Epoch 47/150\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1253 - accuracy: 0.9564\n",
      "Epoch 48/150\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1262 - accuracy: 0.9539\n",
      "Epoch 49/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1148 - accuracy: 0.9570\n",
      "Epoch 50/150\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.1132 - accuracy: 0.9585\n",
      "Epoch 51/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1038 - accuracy: 0.9611\n",
      "Epoch 52/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1011 - accuracy: 0.9608\n",
      "Epoch 53/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1083 - accuracy: 0.9612\n",
      "Epoch 54/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1195 - accuracy: 0.9553\n",
      "Epoch 55/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1071 - accuracy: 0.9605\n",
      "Epoch 56/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0976 - accuracy: 0.9604\n",
      "Epoch 57/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1054 - accuracy: 0.9627\n",
      "Epoch 58/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0927 - accuracy: 0.9630\n",
      "Epoch 59/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1064 - accuracy: 0.9606\n",
      "Epoch 60/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.1077 - accuracy: 0.9590\n",
      "Epoch 61/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0961 - accuracy: 0.9660\n",
      "Epoch 62/150\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.0903 - accuracy: 0.9679\n",
      "Epoch 63/150\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.0904 - accuracy: 0.9636\n",
      "Epoch 64/150\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.0869 - accuracy: 0.9647\n",
      "Epoch 65/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0873 - accuracy: 0.9650\n",
      "Epoch 66/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0883 - accuracy: 0.9662\n",
      "Epoch 67/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0855 - accuracy: 0.9696\n",
      "Epoch 68/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0780 - accuracy: 0.9694\n",
      "Epoch 69/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0780 - accuracy: 0.9682\n",
      "Epoch 70/150\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.0750 - accuracy: 0.9713\n",
      "Epoch 71/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0744 - accuracy: 0.9704\n",
      "Epoch 72/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0778 - accuracy: 0.9676\n",
      "Epoch 73/150\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.0730 - accuracy: 0.9714\n",
      "Epoch 74/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0664 - accuracy: 0.9717\n",
      "Epoch 75/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0790 - accuracy: 0.9677\n",
      "Epoch 76/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0714 - accuracy: 0.9695\n",
      "Epoch 77/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0713 - accuracy: 0.9681\n",
      "Epoch 78/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0680 - accuracy: 0.9737\n",
      "Epoch 79/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0666 - accuracy: 0.9744\n",
      "Epoch 80/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0664 - accuracy: 0.9719\n",
      "Epoch 81/150\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.0712 - accuracy: 0.9670\n",
      "Epoch 82/150\n",
      "153/153 [==============================] - 0s 913us/step - loss: 0.0726 - accuracy: 0.9703\n",
      "Epoch 83/150\n",
      "153/153 [==============================] - 0s 937us/step - loss: 0.0812 - accuracy: 0.9647\n",
      "Epoch 84/150\n",
      "153/153 [==============================] - 0s 955us/step - loss: 0.0687 - accuracy: 0.9679\n",
      "Epoch 85/150\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.0665 - accuracy: 0.9680\n",
      "Epoch 86/150\n",
      "153/153 [==============================] - 0s 946us/step - loss: 0.0694 - accuracy: 0.9692\n",
      "Epoch 87/150\n",
      "153/153 [==============================] - 0s 890us/step - loss: 0.0647 - accuracy: 0.9694\n",
      "Epoch 88/150\n",
      "153/153 [==============================] - 0s 990us/step - loss: 0.0656 - accuracy: 0.9684\n",
      "Epoch 89/150\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.0668 - accuracy: 0.9709\n",
      "Epoch 90/150\n",
      "153/153 [==============================] - 0s 923us/step - loss: 0.0675 - accuracy: 0.9704\n",
      "Epoch 91/150\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 0.0608 - accuracy: 0.9719\n",
      "Epoch 92/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0665 - accuracy: 0.9704\n",
      "Epoch 93/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0680 - accuracy: 0.9675\n",
      "Epoch 94/150\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.0606 - accuracy: 0.9718\n",
      "Epoch 95/150\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.0588 - accuracy: 0.9717\n",
      "Epoch 96/150\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.0606 - accuracy: 0.9719\n",
      "Epoch 97/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0640 - accuracy: 0.9680\n",
      "Epoch 98/150\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.0633 - accuracy: 0.9685\n",
      "Epoch 99/150\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.0547 - accuracy: 0.9765\n",
      "Epoch 100/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0610 - accuracy: 0.9729\n",
      "Epoch 101/150\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 0.0592 - accuracy: 0.9729\n",
      "Epoch 102/150\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.0612 - accuracy: 0.9713\n",
      "Epoch 103/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0576 - accuracy: 0.9734\n",
      "Epoch 104/150\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.0614 - accuracy: 0.9734\n",
      "Epoch 105/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0628 - accuracy: 0.9703\n",
      "Epoch 106/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0557 - accuracy: 0.9738\n",
      "Epoch 107/150\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.0566 - accuracy: 0.9759\n",
      "Epoch 108/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0596 - accuracy: 0.9740\n",
      "Epoch 109/150\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.0546 - accuracy: 0.9725\n",
      "Epoch 110/150\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.0615 - accuracy: 0.9716\n",
      "Epoch 111/150\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.0594 - accuracy: 0.9738\n",
      "Epoch 112/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0593 - accuracy: 0.9713\n",
      "Epoch 113/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0657 - accuracy: 0.9689\n",
      "Epoch 114/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0626 - accuracy: 0.9679\n",
      "Epoch 115/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0584 - accuracy: 0.9742\n",
      "Epoch 116/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0587 - accuracy: 0.9727\n",
      "Epoch 117/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0634 - accuracy: 0.9691\n",
      "Epoch 118/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0534 - accuracy: 0.9778\n",
      "Epoch 119/150\n",
      "153/153 [==============================] - 0s 3ms/step - loss: 0.0588 - accuracy: 0.9735\n",
      "Epoch 120/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0568 - accuracy: 0.9720\n",
      "Epoch 121/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0693 - accuracy: 0.9669\n",
      "Epoch 122/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0547 - accuracy: 0.9753\n",
      "Epoch 123/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0564 - accuracy: 0.9732\n",
      "Epoch 124/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0607 - accuracy: 0.9735\n",
      "Epoch 125/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0551 - accuracy: 0.9729\n",
      "Epoch 126/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0502 - accuracy: 0.9776\n",
      "Epoch 127/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0581 - accuracy: 0.9716\n",
      "Epoch 128/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0572 - accuracy: 0.9728\n",
      "Epoch 129/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0511 - accuracy: 0.9761\n",
      "Epoch 130/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0544 - accuracy: 0.9763\n",
      "Epoch 131/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0609 - accuracy: 0.9706\n",
      "Epoch 132/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0492 - accuracy: 0.9777\n",
      "Epoch 133/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0562 - accuracy: 0.9758\n",
      "Epoch 134/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0589 - accuracy: 0.9718\n",
      "Epoch 135/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0502 - accuracy: 0.9747\n",
      "Epoch 136/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0526 - accuracy: 0.9760\n",
      "Epoch 137/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0579 - accuracy: 0.9730\n",
      "Epoch 138/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0606 - accuracy: 0.9703\n",
      "Epoch 139/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0534 - accuracy: 0.9752\n",
      "Epoch 140/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0538 - accuracy: 0.9740\n",
      "Epoch 141/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0539 - accuracy: 0.9748\n",
      "Epoch 142/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0495 - accuracy: 0.9775\n",
      "Epoch 143/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0516 - accuracy: 0.9762\n",
      "Epoch 144/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0533 - accuracy: 0.9754\n",
      "Epoch 145/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0560 - accuracy: 0.9727\n",
      "Epoch 146/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0565 - accuracy: 0.9725\n",
      "Epoch 147/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0527 - accuracy: 0.9754\n",
      "Epoch 148/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0534 - accuracy: 0.9743\n",
      "Epoch 149/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0564 - accuracy: 0.9760\n",
      "Epoch 150/150\n",
      "153/153 [==============================] - 0s 2ms/step - loss: 0.0602 - accuracy: 0.9698\n",
      "158/158 [==============================] - 0s 2ms/step - loss: 0.0656 - accuracy: 0.9694\n",
      "Accuracy: 96.9444\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(normalized_train_X, Ytrain, epochs=150, batch_size=(int)(len(normalized_train_X)/150))\n",
    "_, accuracy = model.evaluate(normalized_train_X, Ytrain)\n",
    "print('Accuracy: %.4f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of the model on the test set\n",
    "\n",
    "Find the accuracy of your trained model on the test set. Don't forget that you had normalized your training set before training the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "155/155 [==============================] - 1s 2ms/step - loss: 0.0670 - accuracy: 0.9694\n",
      "Epoch 2/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0709 - accuracy: 0.9674\n",
      "Epoch 3/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.9703\n",
      "Epoch 4/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0670 - accuracy: 0.9721\n",
      "Epoch 5/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.9666\n",
      "Epoch 6/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0671 - accuracy: 0.9727\n",
      "Epoch 7/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0730 - accuracy: 0.9701\n",
      "Epoch 8/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0604 - accuracy: 0.9775\n",
      "Epoch 9/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0544 - accuracy: 0.9798\n",
      "Epoch 10/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0701 - accuracy: 0.9674\n",
      "Epoch 11/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0569 - accuracy: 0.9751\n",
      "Epoch 12/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0557 - accuracy: 0.9808\n",
      "Epoch 13/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0853 - accuracy: 0.9641\n",
      "Epoch 14/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0688 - accuracy: 0.9723\n",
      "Epoch 15/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0652 - accuracy: 0.9698\n",
      "Epoch 16/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0619 - accuracy: 0.9749\n",
      "Epoch 17/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0687 - accuracy: 0.9667\n",
      "Epoch 18/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0623 - accuracy: 0.9724\n",
      "Epoch 19/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0616 - accuracy: 0.9769\n",
      "Epoch 20/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0594 - accuracy: 0.9744\n",
      "Epoch 21/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0635 - accuracy: 0.9741\n",
      "Epoch 22/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0664 - accuracy: 0.9653\n",
      "Epoch 23/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0699 - accuracy: 0.9655\n",
      "Epoch 24/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0610 - accuracy: 0.9762\n",
      "Epoch 25/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0641 - accuracy: 0.9729\n",
      "Epoch 26/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0596 - accuracy: 0.9747\n",
      "Epoch 27/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0668 - accuracy: 0.9691\n",
      "Epoch 28/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0706 - accuracy: 0.9692\n",
      "Epoch 29/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0824 - accuracy: 0.9648\n",
      "Epoch 30/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0644 - accuracy: 0.9733\n",
      "Epoch 31/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0620 - accuracy: 0.9743\n",
      "Epoch 32/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0665 - accuracy: 0.9711\n",
      "Epoch 33/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0648 - accuracy: 0.9700\n",
      "Epoch 34/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0684 - accuracy: 0.9712\n",
      "Epoch 35/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0628 - accuracy: 0.9773\n",
      "Epoch 36/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0616 - accuracy: 0.9708\n",
      "Epoch 37/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0665 - accuracy: 0.9706\n",
      "Epoch 38/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0583 - accuracy: 0.9780\n",
      "Epoch 39/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0719 - accuracy: 0.9653\n",
      "Epoch 40/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0555 - accuracy: 0.9785\n",
      "Epoch 41/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0712 - accuracy: 0.9675\n",
      "Epoch 42/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0636 - accuracy: 0.9724\n",
      "Epoch 43/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0584 - accuracy: 0.9752\n",
      "Epoch 44/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0612 - accuracy: 0.9720\n",
      "Epoch 45/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0599 - accuracy: 0.9733\n",
      "Epoch 46/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0509 - accuracy: 0.9795\n",
      "Epoch 47/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0607 - accuracy: 0.9719\n",
      "Epoch 48/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0732 - accuracy: 0.9689\n",
      "Epoch 49/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0599 - accuracy: 0.9740\n",
      "Epoch 50/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0617 - accuracy: 0.9737\n",
      "Epoch 51/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0740 - accuracy: 0.9652\n",
      "Epoch 52/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0537 - accuracy: 0.9781\n",
      "Epoch 53/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0626 - accuracy: 0.9709\n",
      "Epoch 54/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0589 - accuracy: 0.9728\n",
      "Epoch 55/150\n",
      "155/155 [==============================] - 0s 802us/step - loss: 0.0658 - accuracy: 0.9724\n",
      "Epoch 56/150\n",
      "155/155 [==============================] - 0s 832us/step - loss: 0.0597 - accuracy: 0.9754\n",
      "Epoch 57/150\n",
      "155/155 [==============================] - 0s 822us/step - loss: 0.0590 - accuracy: 0.9699\n",
      "Epoch 58/150\n",
      "155/155 [==============================] - 0s 989us/step - loss: 0.0594 - accuracy: 0.9744\n",
      "Epoch 59/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0497 - accuracy: 0.9799\n",
      "Epoch 60/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0663 - accuracy: 0.9720\n",
      "Epoch 61/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0632 - accuracy: 0.9727\n",
      "Epoch 62/150\n",
      "155/155 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9804\n",
      "Epoch 63/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0590 - accuracy: 0.9761\n",
      "Epoch 64/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0571 - accuracy: 0.9745\n",
      "Epoch 65/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0560 - accuracy: 0.9757: 0s - loss: 0.0510 - accuracy\n",
      "Epoch 66/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0598 - accuracy: 0.9750\n",
      "Epoch 67/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0636 - accuracy: 0.9706\n",
      "Epoch 68/150\n",
      "155/155 [==============================] - 0s 901us/step - loss: 0.0577 - accuracy: 0.9759\n",
      "Epoch 69/150\n",
      "155/155 [==============================] - 0s 819us/step - loss: 0.0605 - accuracy: 0.9724\n",
      "Epoch 70/150\n",
      "155/155 [==============================] - 0s 815us/step - loss: 0.0615 - accuracy: 0.9720\n",
      "Epoch 71/150\n",
      "155/155 [==============================] - 0s 895us/step - loss: 0.0538 - accuracy: 0.9753\n",
      "Epoch 72/150\n",
      "155/155 [==============================] - 0s 823us/step - loss: 0.0488 - accuracy: 0.9770\n",
      "Epoch 73/150\n",
      "155/155 [==============================] - 0s 865us/step - loss: 0.0606 - accuracy: 0.9733\n",
      "Epoch 74/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0651 - accuracy: 0.9710\n",
      "Epoch 75/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0587 - accuracy: 0.9712\n",
      "Epoch 76/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0554 - accuracy: 0.9752\n",
      "Epoch 77/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0679 - accuracy: 0.9720\n",
      "Epoch 78/150\n",
      "155/155 [==============================] - 0s 3ms/step - loss: 0.0593 - accuracy: 0.9744\n",
      "Epoch 79/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0643 - accuracy: 0.9723\n",
      "Epoch 80/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0704 - accuracy: 0.9680\n",
      "Epoch 81/150\n",
      "155/155 [==============================] - 0s 912us/step - loss: 0.0588 - accuracy: 0.9796\n",
      "Epoch 82/150\n",
      "155/155 [==============================] - 0s 873us/step - loss: 0.0542 - accuracy: 0.9787\n",
      "Epoch 83/150\n",
      "155/155 [==============================] - 0s 924us/step - loss: 0.0699 - accuracy: 0.9736\n",
      "Epoch 84/150\n",
      "155/155 [==============================] - 0s 882us/step - loss: 0.0550 - accuracy: 0.9753\n",
      "Epoch 85/150\n",
      "155/155 [==============================] - 0s 869us/step - loss: 0.0605 - accuracy: 0.9736\n",
      "Epoch 86/150\n",
      "155/155 [==============================] - 0s 964us/step - loss: 0.0504 - accuracy: 0.9774\n",
      "Epoch 87/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0561 - accuracy: 0.9768\n",
      "Epoch 88/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0545 - accuracy: 0.9778\n",
      "Epoch 89/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0613 - accuracy: 0.9748\n",
      "Epoch 90/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0614 - accuracy: 0.9726\n",
      "Epoch 91/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0575 - accuracy: 0.9751\n",
      "Epoch 92/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0644 - accuracy: 0.9699\n",
      "Epoch 93/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0719 - accuracy: 0.9662\n",
      "Epoch 94/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0634 - accuracy: 0.9750\n",
      "Epoch 95/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0552 - accuracy: 0.9790\n",
      "Epoch 96/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0562 - accuracy: 0.9770\n",
      "Epoch 97/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0635 - accuracy: 0.9715\n",
      "Epoch 98/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0626 - accuracy: 0.9733\n",
      "Epoch 99/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0682 - accuracy: 0.9702\n",
      "Epoch 100/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0576 - accuracy: 0.9760\n",
      "Epoch 101/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0497 - accuracy: 0.9794\n",
      "Epoch 102/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0627 - accuracy: 0.9725\n",
      "Epoch 103/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0632 - accuracy: 0.9735\n",
      "Epoch 104/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0636 - accuracy: 0.9701\n",
      "Epoch 105/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0648 - accuracy: 0.9689\n",
      "Epoch 106/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0537 - accuracy: 0.9789\n",
      "Epoch 107/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0500 - accuracy: 0.9779\n",
      "Epoch 108/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0599 - accuracy: 0.9746\n",
      "Epoch 109/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0657 - accuracy: 0.9706\n",
      "Epoch 110/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0552 - accuracy: 0.9775\n",
      "Epoch 111/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0570 - accuracy: 0.9783\n",
      "Epoch 112/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0598 - accuracy: 0.9723\n",
      "Epoch 113/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0587 - accuracy: 0.9754\n",
      "Epoch 114/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0592 - accuracy: 0.9747\n",
      "Epoch 115/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0617 - accuracy: 0.9733\n",
      "Epoch 116/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0509 - accuracy: 0.9801\n",
      "Epoch 117/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0689 - accuracy: 0.9722\n",
      "Epoch 118/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0549 - accuracy: 0.9765\n",
      "Epoch 119/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0512 - accuracy: 0.9773\n",
      "Epoch 120/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0689 - accuracy: 0.9674\n",
      "Epoch 121/150\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.0650 - accuracy: 0.9710\n",
      "Epoch 122/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0528 - accuracy: 0.9806\n",
      "Epoch 123/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0586 - accuracy: 0.9726\n",
      "Epoch 124/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0551 - accuracy: 0.9772\n",
      "Epoch 125/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0609 - accuracy: 0.9714\n",
      "Epoch 126/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0565 - accuracy: 0.9743\n",
      "Epoch 127/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0571 - accuracy: 0.9753\n",
      "Epoch 128/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0608 - accuracy: 0.9725\n",
      "Epoch 129/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0573 - accuracy: 0.9771\n",
      "Epoch 130/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0606 - accuracy: 0.9705\n",
      "Epoch 131/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0545 - accuracy: 0.9801\n",
      "Epoch 132/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0664 - accuracy: 0.9691\n",
      "Epoch 133/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0585 - accuracy: 0.9744\n",
      "Epoch 134/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0579 - accuracy: 0.9754\n",
      "Epoch 135/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0652 - accuracy: 0.9735\n",
      "Epoch 136/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0643 - accuracy: 0.9717\n",
      "Epoch 137/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0628 - accuracy: 0.9737\n",
      "Epoch 138/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0547 - accuracy: 0.9749\n",
      "Epoch 139/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0520 - accuracy: 0.9785\n",
      "Epoch 140/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0620 - accuracy: 0.9716\n",
      "Epoch 141/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0546 - accuracy: 0.9808\n",
      "Epoch 142/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0635 - accuracy: 0.9704\n",
      "Epoch 143/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0452 - accuracy: 0.9834: 0s - loss: 0.0441 - accuracy: 0.98\n",
      "Epoch 144/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0593 - accuracy: 0.9761\n",
      "Epoch 145/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0598 - accuracy: 0.9769\n",
      "Epoch 146/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0561 - accuracy: 0.9732\n",
      "Epoch 147/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0602 - accuracy: 0.9738: 0s - loss: 0.0634 - accura\n",
      "Epoch 148/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0585 - accuracy: 0.9780\n",
      "Epoch 149/150\n",
      "155/155 [==============================] - 0s 3ms/step - loss: 0.0787 - accuracy: 0.9625\n",
      "Epoch 150/150\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.0585 - accuracy: 0.9749\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0783 - accuracy: 0.9676\n",
      "Accuracy: 96.7593\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(Xtest, Ytest, epochs=150, batch_size=(int)(len(Xtest)/150))\n",
    "_, accuracy = model.evaluate(Xtest, Ytest)\n",
    "print('Accuracy: %.4f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is accuracy really the best metric to evaluate our model on the test set here? What was the proportion of positive samples to negative samples in the dataset? What would have been the accuracy of a model that would have output '0' for any input? By the way, this dataset is for detecting the presence of thyroid in a patient.\n",
    "\n",
    "Now, find out about the metrics - recall, precision and F1-score. Use these metrics to evaluate your model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1974   22]\n",
      " [  79   85]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.99      0.98      1996\n",
      "         1.0       0.79      0.52      0.63       164\n",
      "\n",
      "    accuracy                           0.95      2160\n",
      "   macro avg       0.88      0.75      0.80      2160\n",
      "weighted avg       0.95      0.95      0.95      2160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=1000)\n",
    "mlp.fit(normalized_train_X, Ytrain.ravel())\n",
    "predictions = mlp.predict(Xtest)\n",
    "print(confusion_matrix(Ytest,predictions))\n",
    "print(classification_report(Ytest,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, tune the hyperparameters of your model (like number of layers, number of units in different layers, etc.) to try and do better and better on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Plotting histograms based on prediction values\n",
    "\n",
    "Now, you will plot 2 historgrams of prediction values (0 to 1, or 0% to 100%) on the training data, output by your nn model - one histogram for positive samples and another for negative samples. Plot both the histograms in the same figure. \n",
    "\n",
    "Here's one such plot I made with my nn model:\n",
    "<img src=\"files/index.jpeg\">\n",
    "\n",
    "The blue histogram is for negative samples and the red one is for positive samples (add a legend to the plots, unlike me).\n",
    "\n",
    "Use log-scale on the y-axis (number of occurances in the given predicted value range), like I have done.\n",
    "\n",
    "Now, what can you infer from the histograms you got? How should an ideal pair of histograms look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L = np.array(X)\n",
    "indices0 = [i for i in range(len(L)) if y[i]==0] \n",
    "indices1 = [i for i in range(len(L)) if y[i]==1] \n",
    "L_0present = [L[ind] for ind in indices0]\n",
    "L_0present = np.array(L_0present)\n",
    "L_1present = [L[ind] for ind in indices1]\n",
    "L_1present = np.array(L_1present)\n",
    "prediction0 = model.predict(L_0present)\n",
    "prediction1 = model.predict(L_1present)\n",
    "num_bins = 50\n",
    "n, bins, patches = plt.hist(prediction0, num_bins, facecolor='blue', alpha=0.3, label='negative')\n",
    "plt.yscale('log')\n",
    "num_bins = 50\n",
    "n, bins, patches = plt.hist(prediction1, num_bins, facecolor='brown', alpha=0.5, label='positive')\n",
    "plt.yscale('log')\n",
    "plt.title('Anomaly Score Histogram',fontsize=15)\n",
    "plt.xlabel('Anomaly Score', color = 'crimson',fontsize=15)\n",
    "plt.ylabel('Occurrences', color = 'crimson',fontsize=15)\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An ideal histogram will just be a blue bar on the left (of the number of \"negative\" y entries) \n",
    "#and a brown bar on the right(of the number of \"positive\" y entries)\n",
    "#We see that our histogram achieves close to ideal behaviour for negative entries (because \n",
    "# we can ignore the \"blue bushes\". Why? See below.)\n",
    "#It might seem that the blue bushes amount to much, but since it's a log scale, and since the leftmost blue bar \n",
    "#is so high, the difference is enormous (~200 times). However, the brown bushes are much more significant \n",
    "#(we observe that the brown bush peaks at 0.8, not 1. The inference we can draw is that if the machine tells you\n",
    "#don't have thyroid , you can be ~100% sure it's so, while if it tells you do have thyroid, you can only be \n",
    "#~80% sure, ie:- there's still a 20% chance you may be lucky and don't have thyroid! This model thus suffers\n",
    "#from too many false positives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
